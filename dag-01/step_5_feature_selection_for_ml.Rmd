---
title: "Feature Selection For Machine Learning"
author: "Hicham Zmarrou"
date: "`r Sys.Date()`"
output:
  html_notebook:
    highlight: pygments
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
---

```{r}
suppressMessages(library(tidyverse))

```
___________________________________________________________________________________________________________

The data features that you use to train your machine learning models have a huge influence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance. In this chapter you will discover automatic feature selection techniques
that you can use to prepare your machine learning data in R. After completing this lesson you will know how to use:

1. Univariate Selection.

2. Recursive Feature Elimination.

3. Principle Component Analysis.

4. Feature Importance.

Let's get started.

## Feature Selection
Feature selection is a process where you automatically select those features in your data that
contribute most to the prediction variable or output in which you are interested. Having
irrelevant features in your data can decrease the accuracy of many models, especially linear
algorithms like linear and logistic regression. Three benefits of performing feature selection
before modeling your data are:


+  Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.

+  Improves Accuracy: Less misleading data means modeling accuracy improves.

+ Reduces Training Time: Less data means that algorithms train faster. You can learn more about feature selection with scikit-learn in the article 
Feature selection1.

Each feature selection recipes will use the Pima Indians onset of diabetes dataset.


### Univariate Selection

Statistical tests can be used to select those features that have the strongest relationship with the output variable. As an example, it has been suggested for classification models, that predictors can be filtered by conducting some sort of k-sample test (where k is the number of classes) to see if the mean of the predictor is different between the classes. Wilcoxon tests, t-tests and ANOVA models are sometimes used. Predictors that have statistically significant differences between the classes are then used for modeling.

The `caret` function `sbf` (for selection by filter) can be used to cross-validate such feature selection schemes. Functions can be passed into sbf for the computational components: univariate filtering, model fitting, prediction and performance summaries (details are given below).

The function is applied to the entire training set and also to different resampled versions of the data set. From this, generalizable estimates of performance can be computed that properly take into account the feature selection step. Also, the results of the predictor filters can be tracked over resamples to understand the uncertainty in the filtering.

```{r}
set.seed(7)
# load the library
suppressMessages(library(mlbench))
suppressMessages(library(caret))
# load the data
data <- read_csv("./data/pima_indians_diabetes.csv", col_names = FALSE)
names <- c('preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class')
names(data) <- names
data$class <- factor(data$class)
data       <- as.data.frame(data)

filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", repeats = 5)
set.seed(10)
rfWithFilter <- sbf(data[,1:8], data[,9], sbfControl = filterCtrl)
rfWithFilter

```


In this case, the training set indicated that 7 should be used in the random forest model, but the resampling results indicate that there is some variation (5) in this number. Some of the informative predictors are used, but a few others are erroneous retained.

### Recursive Feature Elimination

The Recursive Feature Elimination (or RFE) is a popular algorithm and works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.

The example below provides an example of the RFE method on the Pima Indians Diabetes dataset. A Random Forest algorithm is used on each iteration to evaluate the model. The algorithm is configured to explore all possible subsets of the attributes. All 8 attributes are selected in this example, although in the plot showing the accuracy of the different attribute subset sizes, we can see that just 4 attributes gives almost comparable results.

```{r}

# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(data[,1:8], data[,9], sizes=c(1:8), rfeControl=control)

# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```
### Principal Component Analysis

Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal components in the transformed result. In the example below, we use PCA and select 3 principal components. Install the two packages as follow:

```{r eval=FALSE, include=FALSE}
install.packages(c("FactoMineR", "factoextra"))
```


Load them in R, by typing this:

```{r}
suppressMessages(library("FactoMineR"))
suppressMessages(library("factoextra"))
respca <- PCA(data[, -9], graph = FALSE)
fviz_eig(respca, addlabels = TRUE, ylim = c(0, 30))
```


The function `PCA` [FactoMineR package] can be used. A simplified format is :

```
PCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)
```

+ X: a data frame. Rows are individuals and columns are numeric variables

+ scale.unit: a logical value. If TRUE, the data are scaled to unit variance before the analysis. This standardization to the same scale avoids some variables to become dominant just because of their large measurement units. It makes variable comparable.

+ ncp: number of dimensions kept in the final results.
+ graph: a logical value. If TRUE a graph is displayed.

### Feature Importance

To evaluate the variable importance (VI) we will use the "Random Uniform Forests", which has a wide selection of instruments for its deep analysis and visualization. As per intent of the package's developers, the main objective of determining the importance of variables is to assess which, when, where and how they affect the problem being solved.

The package provides various importance measures of a variable in depth. We will consider them prior to going into deeper evaluation. 

+ The _Global Variable Importance_ sets variables that reduce the prediction error the utmost, but it doesn't tell us how the important variable affects the responses.

For example, we would like to know, which variables have a stronger influence over the separate class, or what is the interaction between variables. The variable importance is measured by all units and trees, and that allows all variables to have a value, as the cutting points are accidental. Therefore, each variable has equal chances of being selected, but it will be getting the importance, only if it will be the one which mostly reduces the entropy in each node.

+ _Local Variable Importance_ : a predictor is locally important in the first order, if for the same observation and all the trees it is the one with a highest frequency of occurrence in a terminal node.

+ _Partial importance_: partial importance is the way to assess the importance of a covariate when the label Y is fixed (or for realizations of Y above or below a threshold in the case of regression). A predictor is partially important, if for the same observation and one class and at all orders it is the one that has the highest frequency of occurrence at the terminal node.

+ _Interactions_: we want to know, how predictors influence the problem, when we consider them. For example, some variables can have a relative low impact on the problem, but a strong impact on more relevant variables, or a variable can have a lot of interaction with others, which makes this variable influential. Let's define what interaction is. A predictor interacts with another, if on the same observation and for all the trees both have, respectively, first and second highest frequency of occurrence in the terminal node.



+ _Partial dependencies_ : these are the tools that allow to determine, how a variable (or a pair of variables) affects the value of response, knowing the values of all other variables. 

In accordance with the ideas of the Random Uniform Forests package we can determine the importance of a variable based on the following scheme: _Importance = contribution + interaction_, where _contribution_ is the influence of a variable (in respect to influencing all) on prediction errors, and _interaction_ is an impact on other variables.

Let's proceed to experiments

We will divide our data set `data` into the training and testing sets with ratio 2/3, normalize in the range of $[-1,1]$$ and test the model. For separation we will use the `rminer::holdout()` function which will divide the set in two. For normalization we use the `caret::preProcess()` function and the `method = c("spatialSign")`. When training the model the package will automatically parallelize calculations between available processor cores minus one using the `"doParallel"` package. You can indicate a specific number of cores to be used for calculation with the "threads" option.



```{r}
suppressMessages(library(randomUniformForest))
idx <- rminer::holdout(y = data$class)
prep <- caret::preProcess(x = data[idx$tr, -ncol(data)],method = c("spatialSign"))

 x.train <- predict(prep, data[idx$tr, -ncol(data)])
 x.test <- predict(prep, data[idx$ts, -ncol(data)])
 y.train <- data[idx$tr, ncol(data)]
 y.test <- data[idx$ts, ncol(data)]
 ruf <- randomUniformForest( X = x.train, 
                              Y = y.train,
                              xtest = x.test, 
                              ytest = y.test,
                              mtry = 1, ntree = 300,
                              threads = 2, 
                              nodesize = 2
                              )
```

print out the results 




```{r}
print(ruf)
plot(ruf)
```

```{r}
print(ruf)

```


```{r}
plot(ruf)
```

Now we are looking at the global importance of predictors.

```{r}
summary(ruf)
```

We see that all our input variables are significant and important. It is indicated, in which classes the variables appear most frequently.

And some more statistics:

```{r}

pr.ruf <- predict(ruf, x.test, type = "response");
ms.ruf <- model.stats(pr.ruf, y.test)

pr.ruf
ms.ruf
```

If we stop right here, which is normally offered by many packages, we would have to select several predictors with the best indicators of global importance. This choice does not provide good results as it does not take into account the mutual influence of the predictors.

Local importance

```{r}
imp.ruf <- importance(ruf, Xtest = x.test, maxInteractions = 3)

```

From what we see, the importance of variables on the basis of interaction with others highlights the top 10 that don't match the order of global importance. And finally, the importance of variables by classes taking into account their contribution and involvement. Please note that the variable tr, which on the basis of global importance was at the last place and, in theory, should have been abandoned, has actually risen to the sixth place due to the strong interaction.

Thus, top 5 variables:

```{r}
best <- c("mass", "plas", "age", "skin", "preg")
```


```{r}
 x.tr <- x.train[ ,best]
 x.tst <- x.test[ ,best]
 ruf.opt <- randomUniformForest(X = x.tr,
                                Y = y.train,
                                xtest = x.tst, 
                                ytest = y.test,
                                ntree = 300, 
                                mtry = "random",
                                nodesize = 1,
                                threads = 2)
```

print out the results 

```{r}
ruf.opt

```
